# -*- coding: utf-8 -*-
"""IngestIBKR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KGMLvmpsaZsCwT68tBaEYLExa2fz3i5Y
"""

import pandas as pd
import csv
from gsheets_write import GspreadWriter


def ingest_csv_file(csv_filepath: str):
    # native pd.read_csv not used, because of the structure of the file, it will
    # cause error while reading
    overall = []
    with open(csv_filepath, newline="\n", encoding="utf-8-sig") as csvfile:
        spamreader = csv.reader(csvfile, delimiter=",", quotechar='"')
        for row in spamreader:
            overall.append(row)
    df = pd.DataFrame(overall)
    return df


def split_report_into_subsections(csv_not_processed):
    df_grouped = csv_not_processed.groupby(csv_not_processed[0])
    df_separated = {}
    for key, value in df_grouped:
        subsection_processed = process_subsection(value)
        df_separated.update({key: subsection_processed})
    return df_separated


def process_subsection(individual_subsection):
    individual_subsection = individual_subsection.reset_index(drop=True)
    individual_subsection.columns = individual_subsection.iloc[0]
    individual_subsection = individual_subsection.iloc[1:]
    individual_subsection = individual_subsection.dropna(how="all", axis=1)
    individual_subsection = individual_subsection.drop(
        list(individual_subsection.columns)[0], axis=1
    )
    return individual_subsection


def process_trades_section(trades):
    # Indicating Timezone from IBKR is Eastern Timing 0930 onwards.
    trades["Date/Time"] = pd.to_datetime(trades["Date/Time"], errors="coerce")
    trades["Date/Time"] = trades["Date/Time"].dt.tz_localize("EST")
    trades["Date"] = trades["Date/Time"].dt.date
    trades["Date"] = trades["Date"].ffill()
    trades[
        ["Quantity", "T. Price", "Proceeds", "Comm/Fee", "Basis", "Realized P/L"]
    ] = pd.to_numeric(
        trades[
            ["Quantity", "T. Price", "Proceeds", "Comm/Fee", "Basis", "Realized P/L"]
        ].stack()
    ).unstack()
    return trades


def process_existing_entries(trading_journal_entries):
    existing_entries = pd.DataFrame(trading_journal_entries.get_all_records())
    existing_entries["Date/Time"] = pd.to_datetime(
        existing_entries["Date/Time"], errors="coerce"
    )
    # existing_entries['Date/Time'] = existing_entries['Date/Time'].dt.tz_localize('EST')
    existing_entries["Date"] = existing_entries["Date/Time"].dt.date
    existing_entries = existing_entries[
        existing_entries["DataDiscriminator"] == "Order"
    ]
    return existing_entries


def concat_new_entries(existing_entries, new_trades_entries):
    existing_columns = existing_entries.columns
    new_entries_columns = [
        column for column in new_trades_entries.columns if column in existing_columns
    ]
    new_trades_entries = new_trades_entries.loc[:, new_entries_columns]
    updated_entries = pd.concat([existing_entries, new_trades_entries], axis=0)
    return updated_entries


def prepare_datatype_for_updating(updated_entries):
    updated_entries["Date/Time"] = updated_entries["Date/Time"].astype(str)
    updated_entries["Date"] = updated_entries["Date"].astype(str)
    updated_entries = updated_entries.fillna("")
    return updated_entries


def update_google_sheets(trading_journal_entries, updated_entries):
    trading_journal_entries.clear()
    trading_journal_entries.update(
        [updated_entries.columns.values.tolist()] + updated_entries.values.tolist()
    )


def clean_entries(existing_entries):
    existing_entries["columns_filled"] = (
        existing_entries.replace("", None).notna().sum(axis=1)
    )
    existing_entries = existing_entries.sort_values(
        ["Asset Category", "Symbol", "Date/Time", "columns_filled"],
        ascending=[True, True, True, False],
    )
    before_cleaning_length = len(existing_entries)
    existing_entries = existing_entries.drop_duplicates(
        subset=["Asset Category", "Symbol", "Date/Time"], keep="first"
    )
    existing_entries = existing_entries.drop(["columns_filled"], axis=1)
    after_cleaning_length = len(existing_entries)
    if before_cleaning_length != after_cleaning_length:
        print("Unclean Entries Removed")
    return existing_entries


def main(csv_file_path):

    csv_ingested = ingest_csv_file(f"{csv_file_path}")
    csv_sectioned = split_report_into_subsections(csv_ingested)

    if "Trades" in csv_sectioned.keys():
        trades = csv_sectioned["Trades"]
        print(f"Processing: {csv_file_path}")
        trades = process_trades_section(trades)

        trades_entries = trades[trades["DataDiscriminator"] == "Order"]
        # For future use. E.g Aggregating by dates, for visualization purposes
        trades_subtotal = trades[trades["Header"] == "SubTotal"]

        # TODO: Put URL and sheetname into config file
        trading_journal_url = "https://docs.google.com/spreadsheets/d/1NNa_qwBM5vsuYnZfrhHwTiKaWtSJwefjXPDEudroMI4/edit#gid=1017237349"
        gsw = GspreadWriter()
        gc = gsw.authenticate_gspread()
        trading_journal_sh = gc.open_by_url(trading_journal_url)
        trading_journal_entries = trading_journal_sh.worksheet("Sheet13")
        existing_entries = process_existing_entries(trading_journal_entries)
        updated_entries = concat_new_entries(existing_entries, trades_entries)
        updated_entries = clean_entries(updated_entries)
        updated_entries = updated_entries.sort_values(["Date/Time"])
        updated_entries = prepare_datatype_for_updating(updated_entries)
        update_google_sheets(trading_journal_entries, updated_entries)
    else:
        print("No Trades")
